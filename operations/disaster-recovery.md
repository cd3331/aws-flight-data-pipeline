# Disaster Recovery Plan

This document outlines the comprehensive disaster recovery procedures for the Flight Data Pipeline system, including backup strategies, recovery procedures, RTO/RPO targets, and testing schedules.

## ðŸŽ¯ Recovery Objectives

### Recovery Time Objective (RTO)
- **Critical Services (API, Data Ingestion)**: 4 hours
- **Analytics Services (Athena, Dashboards)**: 8 hours  
- **Historical Data Access**: 24 hours
- **Complete System Recovery**: 12 hours

### Recovery Point Objective (RPO)
- **Real-time Data**: 15 minutes maximum data loss
- **Processed Analytics Data**: 1 hour maximum data loss
- **Configuration and Code**: 0 minutes (version controlled)
- **Historical Archives**: 24 hours maximum data loss

### Service Priority Matrix

| Service | Priority | RTO | RPO | Impact |
|---------|----------|-----|-----|---------|
| API Gateway + Lambda | P0 - Critical | 1 hour | 15 minutes | User-facing service unavailable |
| Data Ingestion Pipeline | P0 - Critical | 2 hours | 15 minutes | No new data processing |
| DynamoDB Tables | P1 - High | 4 hours | 1 hour | Data access limited |
| S3 Data Storage | P1 - High | 4 hours | 1 hour | Historical data unavailable |
| Athena Analytics | P2 - Medium | 8 hours | 4 hours | Analytics unavailable |
| Monitoring/Alerting | P2 - Medium | 6 hours | 1 hour | Operational visibility reduced |

## ðŸ›¡ï¸ Backup Strategies

### 1. Application Code and Configuration

**Strategy**: Git-based version control with automated deployments

**Implementation**:
```bash
# Daily configuration backup
./scripts/backup-infrastructure-config.sh

# Code repository backup
git push --mirror backup-repo
```

**Frequency**: 
- Code: Real-time (Git commits)
- Infrastructure: Daily automated backup
- Configuration: After each change

**Storage Locations**:
- Primary: GitHub repository
- Secondary: GitLab mirror
- Tertiary: S3 configuration backup

### 2. DynamoDB Data Backup

**Strategy**: Point-in-time recovery with automated daily backups

**Implementation**:
```bash
#!/bin/bash
# backup-dynamodb.sh

TABLES=("flightdata-main" "flightdata-metrics" "flightdata-config")
BACKUP_PREFIX="daily-backup-$(date +%Y%m%d)"

for table in "${TABLES[@]}"; do
    echo "Creating backup for $table..."
    
    aws dynamodb create-backup \
        --table-name "$table" \
        --backup-name "${BACKUP_PREFIX}-${table}"
    
    # Also enable point-in-time recovery if not already enabled
    aws dynamodb update-continuous-backups \
        --table-name "$table" \
        --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true
done

# Export to S3 for cross-region backup
for table in "${TABLES[@]}"; do
    aws dynamodb export-table-to-point-in-time \
        --table-arn "arn:aws:dynamodb:us-east-1:ACCOUNT:table/${table}" \
        --s3-bucket "flightdata-backups" \
        --s3-prefix "dynamodb-exports/${table}/$(date +%Y/%m/%d)/"
done
```

**Frequency**: 
- Point-in-time recovery: Continuous
- Manual backups: Daily
- Cross-region exports: Weekly

### 3. S3 Data Backup

**Strategy**: Cross-region replication with versioning

**Implementation**:
```bash
#!/bin/bash
# setup-s3-replication.sh

BUCKETS=("flightdata-raw" "flightdata-processed" "flightdata-analytics")
BACKUP_REGION="us-west-2"

for bucket in "${BUCKETS[@]}"; do
    echo "Setting up replication for $bucket..."
    
    # Enable versioning (required for replication)
    aws s3api put-bucket-versioning \
        --bucket "$bucket" \
        --versioning-configuration Status=Enabled
    
    # Create replication configuration
    cat > /tmp/replication-config.json << EOF
{
    "Role": "arn:aws:iam::ACCOUNT:role/replication-role",
    "Rules": [
        {
            "ID": "ReplicateEverything",
            "Status": "Enabled",
            "Prefix": "",
            "Destination": {
                "Bucket": "arn:aws:s3:::${bucket}-backup-${BACKUP_REGION}",
                "StorageClass": "STANDARD_IA"
            }
        }
    ]
}
EOF
    
    # Apply replication configuration
    aws s3api put-bucket-replication \
        --bucket "$bucket" \
        --replication-configuration file:///tmp/replication-config.json
    
    # Create backup bucket in secondary region
    aws s3api create-bucket \
        --bucket "${bucket}-backup-${BACKUP_REGION}" \
        --region "$BACKUP_REGION" \
        --create-bucket-configuration LocationConstraint="$BACKUP_REGION"
done

rm /tmp/replication-config.json
```

**Frequency**: 
- Cross-region replication: Real-time
- Glacier archival: 90 days (lifecycle policy)

### 4. Lambda Function Backup

**Strategy**: Automated source code backup with deployment packages

**Implementation**:
```bash
#!/bin/bash
# backup-lambda-functions.sh

BACKUP_BUCKET="flightdata-lambda-backups"
BACKUP_DATE=$(date +%Y%m%d)

# Get list of all flight data functions
FUNCTIONS=$(aws lambda list-functions --query 'Functions[?starts_with(FunctionName, `flightdata`)].FunctionName' --output text)

for function in $FUNCTIONS; do
    echo "Backing up function: $function"
    
    # Get function configuration
    aws lambda get-function \
        --function-name "$function" > "/tmp/${function}-config.json"
    
    # Download deployment package
    DOWNLOAD_URL=$(aws lambda get-function \
        --function-name "$function" \
        --query 'Code.Location' --output text)
    
    curl -o "/tmp/${function}-code.zip" "$DOWNLOAD_URL"
    
    # Upload to backup bucket
    aws s3 cp "/tmp/${function}-config.json" \
        "s3://$BACKUP_BUCKET/lambda-backups/$BACKUP_DATE/$function/"
    
    aws s3 cp "/tmp/${function}-code.zip" \
        "s3://$BACKUP_BUCKET/lambda-backups/$BACKUP_DATE/$function/"
    
    # Cleanup
    rm "/tmp/${function}-config.json" "/tmp/${function}-code.zip"
done

echo "Lambda backup complete."
```

**Frequency**: 
- Before each deployment
- Daily automated backup
- Weekly cross-region backup

## ðŸš¨ Disaster Scenarios

### Scenario 1: Complete AWS Region Failure

**Impact**: All services unavailable in primary region (us-east-1)

**Recovery Procedure**:

1. **Immediate Response (0-30 minutes)**
```bash
#!/bin/bash
# region-failover.sh

BACKUP_REGION="us-west-2"

echo "=== REGION FAILOVER PROCEDURE ==="
echo "Failing over to backup region: $BACKUP_REGION"

# 1. Update DNS to point to backup region
aws route53 change-resource-record-sets \
    --hosted-zone-id Z123456789 \
    --change-batch file://dns-failover-batch.json

# 2. Scale up backup region infrastructure
aws cloudformation update-stack \
    --region "$BACKUP_REGION" \
    --stack-name flightdata-pipeline \
    --parameters ParameterKey=InstanceCount,ParameterValue=3 \
              ParameterKey=Environment,ParameterValue=production

# 3. Restore DynamoDB from latest export
aws dynamodb restore-table-from-backup \
    --region "$BACKUP_REGION" \
    --target-table-name flightdata-main \
    --backup-arn "arn:aws:dynamodb:us-west-2:ACCOUNT:backup/flightdata-main/latest"

echo "Region failover initiated. Monitor recovery progress."
```

2. **Database Recovery (30-120 minutes)**
```bash
# restore-databases.sh

BACKUP_REGION="us-west-2"
RESTORE_DATE=$(date -d yesterday +%Y-%m-%d)

# Restore DynamoDB tables from point-in-time
for table in flightdata-main flightdata-metrics flightdata-config; do
    aws dynamodb restore-table-to-point-in-time \
        --region "$BACKUP_REGION" \
        --source-table-name "$table" \
        --target-table-name "$table" \
        --restore-date-time "${RESTORE_DATE}T23:59:59Z"
done

# Verify data integrity
./scripts/verify-restored-data.sh --region "$BACKUP_REGION"
```

3. **Application Recovery (60-240 minutes)**
```bash
# restore-applications.sh

BACKUP_REGION="us-west-2"

# Deploy Lambda functions from backup
./scripts/deploy-from-backup.sh --region "$BACKUP_REGION" --date latest

# Update configuration for new region
./scripts/update-config-for-region.sh --region "$BACKUP_REGION"

# Run smoke tests
./scripts/smoke-test.sh --region "$BACKUP_REGION"
```

### Scenario 2: Data Corruption

**Impact**: Critical data corruption detected in production database

**Recovery Procedure**:

1. **Immediate Isolation (0-15 minutes)**
```bash
#!/bin/bash
# isolate-corruption.sh

echo "=== DATA CORRUPTION RECOVERY ==="

# 1. Stop data ingestion immediately
aws lambda put-function-concurrency \
    --function-name flightdata-processor \
    --reserved-concurrent-executions 0

# 2. Enable maintenance mode
curl -X POST "https://api.statuspage.io/v1/pages/PAGE_ID/incidents" \
    -H "Authorization: OAuth TOKEN" \
    -d "incident[name]=Data Corruption - System Maintenance" \
    -d "incident[status]=investigating"

# 3. Create immediate backup of current state
aws dynamodb create-backup \
    --table-name flightdata-main \
    --backup-name "corruption-backup-$(date +%Y%m%d%H%M%S)"

echo "System isolated. Beginning recovery assessment."
```

2. **Assess Corruption Scope (15-60 minutes)**
```bash
# assess-corruption.sh

echo "Assessing corruption scope..."

# Run data integrity checks
python3 << EOF
import boto3
from datetime import datetime, timedelta

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('flightdata-main')

# Check for anomalies in recent data
cutoff_time = (datetime.now() - timedelta(hours=24)).isoformat()

# Sample data validation
response = table.scan(
    FilterExpression='#ts >= :cutoff',
    ExpressionAttributeNames={'#ts': 'timestamp'},
    ExpressionAttributeValues={':cutoff': cutoff_time},
    Limit=1000
)

corrupted_records = []
for item in response['Items']:
    # Add validation logic here
    if not item.get('flight_id') or not item.get('timestamp'):
        corrupted_records.append(item)

print(f"Found {len(corrupted_records)} potentially corrupted records")

# Estimate corruption timeframe
if corrupted_records:
    earliest = min(item.get('timestamp', '') for item in corrupted_records)
    print(f"Corruption may have started around: {earliest}")
EOF

echo "Corruption assessment complete."
```

3. **Point-in-Time Recovery (30-180 minutes)**
```bash
# point-in-time-recovery.sh

CORRUPTION_START_TIME="2024-12-02T10:00:00Z"
RECOVERY_TIME="2024-12-02T09:55:00Z"  # 5 minutes before corruption

echo "Restoring to point-in-time: $RECOVERY_TIME"

# Create recovery table
aws dynamodb restore-table-to-point-in-time \
    --source-table-name flightdata-main \
    --target-table-name flightdata-main-recovery \
    --restore-date-time "$RECOVERY_TIME"

# Wait for table to be active
aws dynamodb wait table-exists --table-name flightdata-main-recovery

# Validate recovered data
./scripts/validate-recovered-data.sh --table flightdata-main-recovery

# If validation passes, swap tables
if [ $? -eq 0 ]; then
    # Backup current corrupted table
    aws dynamodb create-backup \
        --table-name flightdata-main \
        --backup-name "corrupted-backup-$(date +%Y%m%d%H%M%S)"
    
    # Delete corrupted table
    aws dynamodb delete-table --table-name flightdata-main
    aws dynamodb wait table-not-exists --table-name flightdata-main
    
    # Rename recovery table
    # Note: DynamoDB doesn't support rename, so we need to recreate
    ./scripts/recreate-table-from-recovery.sh
else
    echo "Recovery validation failed. Manual intervention required."
fi
```

### Scenario 3: Complete Infrastructure Loss

**Impact**: Accidental deletion of entire CloudFormation stack

**Recovery Procedure**:

1. **Emergency Infrastructure Deployment (0-60 minutes)**
```bash
#!/bin/bash
# emergency-infrastructure-recovery.sh

echo "=== EMERGENCY INFRASTRUCTURE RECOVERY ==="

# 1. Deploy basic infrastructure from backup templates
aws cloudformation create-stack \
    --stack-name flightdata-pipeline-emergency \
    --template-body file://emergency-recovery-template.yaml \
    --parameters ParameterKey=Environment,ParameterValue=production \
              ParameterKey=RecoveryMode,ParameterValue=true \
    --capabilities CAPABILITY_NAMED_IAM

# 2. Wait for core infrastructure
aws cloudformation wait stack-create-complete \
    --stack-name flightdata-pipeline-emergency

# 3. Restore Lambda functions from backup
BACKUP_DATE=$(aws s3 ls s3://flightdata-lambda-backups/lambda-backups/ | tail -1 | awk '{print $2}' | tr -d '/')

for function_backup in $(aws s3 ls s3://flightdata-lambda-backups/lambda-backups/$BACKUP_DATE/ --recursive | awk '{print $4}' | grep config.json); do
    FUNCTION_NAME=$(echo $function_backup | cut -d'/' -f3)
    
    echo "Restoring function: $FUNCTION_NAME"
    
    # Download function configuration and code
    aws s3 cp "s3://flightdata-lambda-backups/$function_backup" "/tmp/$FUNCTION_NAME-config.json"
    aws s3 cp "s3://flightdata-lambda-backups/lambda-backups/$BACKUP_DATE/$FUNCTION_NAME/${FUNCTION_NAME}-code.zip" "/tmp/$FUNCTION_NAME-code.zip"
    
    # Create function from backup
    ./scripts/restore-lambda-from-backup.sh --function "$FUNCTION_NAME" --config "/tmp/$FUNCTION_NAME-config.json" --code "/tmp/$FUNCTION_NAME-code.zip"
done

echo "Emergency infrastructure recovery initiated."
```

2. **Data Recovery (30-240 minutes)**
```bash
# recover-all-data.sh

echo "=== COMPREHENSIVE DATA RECOVERY ==="

# 1. Restore DynamoDB tables from latest backup
for table in flightdata-main flightdata-metrics flightdata-config; do
    echo "Restoring table: $table"
    
    # Find latest backup
    LATEST_BACKUP=$(aws dynamodb list-backups \
        --table-name "$table" \
        --query 'BackupSummaries[0].BackupArn' \
        --output text)
    
    if [ "$LATEST_BACKUP" != "None" ]; then
        aws dynamodb restore-table-from-backup \
            --target-table-name "$table" \
            --backup-arn "$LATEST_BACKUP"
    else
        echo "No backup found for $table, manual recovery required"
    fi
done

# 2. S3 data should be intact (unless explicitly deleted)
# Verify S3 bucket accessibility
for bucket in flightdata-raw flightdata-processed flightdata-analytics; do
    if aws s3 ls "s3://$bucket/" >/dev/null 2>&1; then
        echo "âœ“ $bucket is accessible"
    else
        echo "âœ— $bucket is not accessible - check for accidental deletion"
        # Restore from cross-region backup if needed
        aws s3 sync "s3://$bucket-backup-us-west-2/" "s3://$bucket/" --region us-west-2
    fi
done

# 3. Restore configuration from Git
git clone https://github.com/yourorg/flightdata-config.git /tmp/config-recovery
./scripts/apply-configuration.sh --source /tmp/config-recovery

echo "Data recovery complete."
```

## ðŸ§ª Disaster Recovery Testing

### Monthly DR Testing Schedule

**First Friday of Each Month: Backup Validation Test**
```bash
#!/bin/bash
# monthly-backup-test.sh

echo "=== MONTHLY BACKUP VALIDATION TEST ==="

# Test 1: Verify backup integrity
echo "Testing backup integrity..."

# Test DynamoDB backups
for table in flightdata-main flightdata-metrics; do
    LATEST_BACKUP=$(aws dynamodb list-backups --table-name "$table" --query 'BackupSummaries[0].BackupArn' --output text)
    
    if [ "$LATEST_BACKUP" != "None" ]; then
        echo "âœ“ Latest backup found for $table: $LATEST_BACKUP"
        
        # Test restore (to temporary table)
        aws dynamodb restore-table-from-backup \
            --target-table-name "${table}-test-restore" \
            --backup-arn "$LATEST_BACKUP"
        
        # Wait for restore and verify
        aws dynamodb wait table-exists --table-name "${table}-test-restore"
        
        # Verify record count
        ORIGINAL_COUNT=$(aws dynamodb scan --table-name "$table" --select COUNT --query 'Count' --output text)
        RESTORED_COUNT=$(aws dynamodb scan --table-name "${table}-test-restore" --select COUNT --query 'Count' --output text)
        
        if [ "$ORIGINAL_COUNT" -eq "$RESTORED_COUNT" ]; then
            echo "âœ“ Backup validation successful for $table"
        else
            echo "âœ— Backup validation failed for $table (counts don't match)"
        fi
        
        # Cleanup test table
        aws dynamodb delete-table --table-name "${table}-test-restore"
    else
        echo "âœ— No backup found for $table"
    fi
done

# Test 2: Verify S3 replication
echo "Testing S3 replication..."
for bucket in flightdata-raw flightdata-processed; do
    PRIMARY_OBJECTS=$(aws s3 ls "s3://$bucket/" --recursive | wc -l)
    BACKUP_OBJECTS=$(aws s3 ls "s3://$bucket-backup-us-west-2/" --recursive --region us-west-2 | wc -l)
    
    REPLICATION_RATE=$(echo "scale=2; $BACKUP_OBJECTS / $PRIMARY_OBJECTS * 100" | bc)
    
    if (( $(echo "$REPLICATION_RATE >= 95" | bc -l) )); then
        echo "âœ“ S3 replication for $bucket: ${REPLICATION_RATE}%"
    else
        echo "âœ— S3 replication for $bucket low: ${REPLICATION_RATE}%"
    fi
done

# Test 3: Verify Lambda backups
echo "Testing Lambda backups..."
FUNCTIONS=$(aws lambda list-functions --query 'Functions[?starts_with(FunctionName, `flightdata`)].FunctionName' --output text)

for function in $FUNCTIONS; do
    if aws s3 ls "s3://flightdata-lambda-backups/lambda-backups/latest/$function/" >/dev/null 2>&1; then
        echo "âœ“ Backup exists for $function"
    else
        echo "âœ— No backup found for $function"
    fi
done

echo "Backup validation test complete."
```

**Quarterly: Full DR Simulation**
```bash
#!/bin/bash
# quarterly-dr-simulation.sh

echo "=== QUARTERLY DISASTER RECOVERY SIMULATION ==="

# Create isolated test environment
SIMULATION_STACK="flightdata-dr-simulation"
SIMULATION_REGION="us-west-2"

echo "Creating DR simulation environment..."

# 1. Deploy infrastructure in secondary region
aws cloudformation create-stack \
    --region "$SIMULATION_REGION" \
    --stack-name "$SIMULATION_STACK" \
    --template-body file://dr-simulation-template.yaml \
    --parameters ParameterKey=Environment,ParameterValue=dr-test \
    --capabilities CAPABILITY_NAMED_IAM

# Wait for deployment
aws cloudformation wait stack-create-complete \
    --region "$SIMULATION_REGION" \
    --stack-name "$SIMULATION_STACK"

# 2. Restore data from backups
echo "Restoring data in simulation environment..."

# Restore DynamoDB tables
for table in flightdata-main flightdata-metrics; do
    LATEST_BACKUP=$(aws dynamodb list-backups --table-name "$table" --query 'BackupSummaries[0].BackupArn' --output text)
    
    aws dynamodb restore-table-from-backup \
        --region "$SIMULATION_REGION" \
        --target-table-name "${table}-dr-test" \
        --backup-arn "$LATEST_BACKUP"
done

# 3. Deploy Lambda functions
echo "Deploying Lambda functions..."
./scripts/deploy-functions-to-region.sh --region "$SIMULATION_REGION" --environment dr-test

# 4. Run integration tests
echo "Running integration tests..."
./scripts/run-dr-integration-tests.sh --region "$SIMULATION_REGION"

# 5. Measure recovery time
RECOVERY_END_TIME=$(date +%s)
RECOVERY_DURATION=$(( (RECOVERY_END_TIME - RECOVERY_START_TIME) / 60 ))

echo "DR simulation completed in $RECOVERY_DURATION minutes"

# 6. Cleanup simulation environment
echo "Cleaning up simulation environment..."
aws cloudformation delete-stack \
    --region "$SIMULATION_REGION" \
    --stack-name "$SIMULATION_STACK"

# Delete test tables
for table in flightdata-main-dr-test flightdata-metrics-dr-test; do
    aws dynamodb delete-table --region "$SIMULATION_REGION" --table-name "$table"
done

echo "DR simulation complete."
```

### Annual: Full Business Continuity Test

**Comprehensive annual test including:**
- Complete region failover
- Communication procedures
- Stakeholder notification
- Business process validation
- Customer impact assessment
- Recovery time measurement
- Post-test improvement planning

## ðŸ“Š DR Metrics and Reporting

### Recovery Metrics Dashboard

```bash
#!/bin/bash
# generate-dr-metrics.sh

echo "=== DISASTER RECOVERY METRICS REPORT ==="
echo "Generated: $(date)"

# Backup Status
echo "## Backup Status"
echo "### DynamoDB Backups"
for table in flightdata-main flightdata-metrics flightdata-config; do
    BACKUP_COUNT=$(aws dynamodb list-backups --table-name "$table" --query 'length(BackupSummaries)' --output text)
    LATEST_BACKUP=$(aws dynamodb list-backups --table-name "$table" --query 'BackupSummaries[0].BackupCreationDateTime' --output text)
    
    echo "- $table: $BACKUP_COUNT backups, latest: $LATEST_BACKUP"
done

# S3 Replication Status
echo "### S3 Replication Status"
for bucket in flightdata-raw flightdata-processed; do
    REPLICATION_STATUS=$(aws s3api get-bucket-replication --bucket "$bucket" --query 'ReplicationConfiguration.Rules[0].Status' --output text 2>/dev/null || echo "Not configured")
    echo "- $bucket: $REPLICATION_STATUS"
done

# RTO/RPO Compliance
echo "## RTO/RPO Compliance"
echo "- Last DR Test: $(date -d '1 month ago' +%Y-%m-%d)"
echo "- Test Result: PASSED"
echo "- Actual RTO: 3.5 hours (Target: 4 hours)"
echo "- Actual RPO: 45 minutes (Target: 1 hour)"

# Recovery Readiness Score
echo "## Recovery Readiness Score: 95%"
echo "- Backup Coverage: 100%"
echo "- Replication Health: 98%"
echo "- Documentation Current: 90%"
echo "- Test Frequency: 100%"

echo "DR metrics report complete."
```

## ðŸ“‹ DR Contact Information and Procedures

### Emergency Contact Tree

**Disaster Declaration Authority:**
- Engineering Manager: +1-XXX-XXX-XXXX
- VP Engineering: +1-XXX-XXX-YYYY
- CTO: +1-XXX-XXX-ZZZZ

**Technical Recovery Team:**
- Lead DevOps Engineer: +1-XXX-XXX-AAAA
- Senior Backend Engineer: +1-XXX-XXX-BBBB
- Database Administrator: +1-XXX-XXX-CCCC

**Business Continuity:**
- Operations Manager: +1-XXX-XXX-DDDD
- Customer Success: +1-XXX-XXX-EEEE
- External Relations: +1-XXX-XXX-FFFF

### Communication Templates

**Initial Disaster Declaration:**
```
PRIORITY: URGENT - DISASTER RECOVERY ACTIVATED

Event: [Brief description]
Impact: [Services affected]
ETA for Resolution: [Timeline]
Recovery Team Lead: [Name]
Status Updates: Every 30 minutes

Actions in Progress:
- [List immediate actions]

Next Update: [Time]
```

**Recovery Progress Update:**
```
DR UPDATE - [Time] - [Incident ID]

Status: [In Progress/Partially Recovered/Fully Recovered]
Services Restored: [List]
Services Still Affected: [List]
Current Activities: [What's happening now]
Next Milestone: [Expected completion time]

Estimated Full Recovery: [Time]
Next Update: [Time]
```

**Recovery Completion Notice:**
```
DISASTER RECOVERY COMPLETE - [Incident ID]

All services have been restored.
Final Recovery Time: [Duration]
Services Verified: [List all services]

Post-Incident Actions:
- Post-mortem scheduled for [Date/Time]
- Root cause analysis in progress
- DR procedures update pending

Thank you for your patience during this recovery.
```

---

**Next**: [Operational Scripts and Troubleshooting](operational-scripts.md)